<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Dive into DeepLearning" /><meta property="og:locale" content="en" /><meta name="description" content="數學符號說明 $z = g \circ f $就是z(x) = g(f(x))，也就是$y = f(x), z = g(y)$" /><meta property="og:description" content="數學符號說明 $z = g \circ f $就是z(x) = g(f(x))，也就是$y = f(x), z = g(y)$" /><link rel="canonical" href="https://jenhaoyang.github.io//posts/DiveIntoDeepLearning/" /><meta property="og:url" content="https://jenhaoyang.github.io//posts/DiveIntoDeepLearning/" /><meta property="og:site_name" content="Steven" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-07-05T16:50:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Dive into DeepLearning" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-12T11:04:27+08:00","datePublished":"2023-07-05T16:50:00+08:00","description":"數學符號說明 $z = g \\circ f $就是z(x) = g(f(x))，也就是$y = f(x), z = g(y)$","headline":"Dive into DeepLearning","mainEntityOfPage":{"@type":"WebPage","@id":"https://jenhaoyang.github.io//posts/DiveIntoDeepLearning/"},"url":"https://jenhaoyang.github.io//posts/DiveIntoDeepLearning/"}</script><title>Dive into DeepLearning | Steven</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Steven"><meta name="application-name" content="Steven"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Steven</a></div><div class="site-subtitle font-italic">深度學習、機器學習、MLOps</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/jenhaoyang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['steven05yang','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a> <a href="https://www.linkedin.com/in/jenhaoyang/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Dive into DeepLearning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Dive into DeepLearning</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1688547000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 5, 2023 </em> </span> <span> Updated <em class="" data-ts="1697079867" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 12, 2023 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/jenhaoyang">Steven Yang</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3356 words"> <em>18 min</em> read</span></div></div></div><div class="post-content"><h1 id="數學符號說明">數學符號說明</h1><p>$z = g \circ f $就是z(x) = g(f(x))，也就是$y = f(x), z = g(y)$</p><p>https://math.stackexchange.com/a/1092727 $x \in R$ : x是一個(一維)實數純量，例如 $x =-2$ 或 $x=42$ $x \in R^{n*d}$</p><h1 id="ch2">Ch2</h1><h2 id="25-automatic-diff-erentiation"><span class="mr-2">2.5 Automatic Diff erentiation</span><a href="#25-automatic-diff-erentiation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>現代的深度學習都有提供automatic differentiation(autograd)和backpropagation的功能，下面將介紹如何使用</p><h3 id="252-backward-for-non-scalar-variables"><span class="mr-2">2.5.2 Backward for Non-Scalar Variables</span><a href="#252-backward-for-non-scalar-variables" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>為什麼要先sum()之後才backpropagation?</p><p>cs231n http://cs231n.stanford.edu/handouts/derivatives.pdf</p><ul><li><p>Scalar in Scalar out: 利用chain rule就可以計算backpropagation當函式為$f, g : R \rightarrow R$且$z = (g \circ f)(x)$則$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}$。這告訴我們我們移動x一點點$\Delta_x$則y移動的量如下\(\Delta_y=\frac{\partial y}{\partial x}\Delta_x\)，而z移動的量為\(\frac{\partial z}{\partial y}\Delta_y=\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\Delta_x\)</p><li><p>Vector in, scalar out: 利用Gradient可以計算backpropagation 當函式為$f : R^N \rightarrow R$且$x \in R^N$也就是說x是一個vector，而$\nabla_xf(x) \in R^N$，也就是Gradient的結果也是一個vector。繼續利用前面chain rule的概念\(x\rightarrow x+\Delta_x \Rightarrow y \rightarrow \approx y+\frac{\partial y}{\partial x} \cdot \Delta_x\)不過現在的狀況$x$, $\Delta_x$, \frac{\partial y}{\partial x} 都是vector，而兩個vector的內積剛好是scalar</p><li><p>Vector in, Vector out 現在函式$f:R^N \rightarrow R^M$</p><li><p>Jacobian linear map matrix代表了linear map，而determinant代表linear map之後面積放大的倍率，而負的determinant代表座標方向反轉</p><ol><li>微分不應該單純的想成斜率，積分也不應該單純想成面積。而是用另以種角度來想，微分代表在某一個點上，把它放很大來看之後他的線性映射是如何(linear map)</ol></ul><ol><li>對二維函式來說，Jacobian matrix就是在(a, b)附近的linear map</ol><p>https://youtu.be/wCZ1VEmVjVo <br /> https://youtu.be/CfW845LNObM<br /> https://angeloyeo.github.io/2020/07/24/Jacobian_en.html</p><h1 id="ch7">CH7</h1><h3 id="721-the-cross-correlation-operation"><span class="mr-2">7.2.1 The Cross-Correlation Operation</span><a href="#721-the-cross-correlation-operation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>經過Cross-Correlation Operation後，輸出的tensor尺寸為 $(n_h − k_h + 1 ) × (n_w − k_w + 1 )$</p><h3 id="722-convolutional-layers"><span class="mr-2">7.2.2 Convolutional Layers</span><a href="#722-convolutional-layers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Convolutional Layers就是經過Cross-Correlation Operation之後的tensor對每一個element都加上一個bias。Conv的kernel如同前面MLP的權重，初始化的時候我們是用亂數初始化</p><h3 id="723-object-edge-detection-in-images"><span class="mr-2">7.2.3 Object Edge Detection in Images</span><a href="#723-object-edge-detection-in-images" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>已知一個人工製作的邊緣偵測器kernel為[1, -1]可以偵測垂直線，等一下會嘗試讓電腦自己學習出這個kernel</p><h3 id="724-learning-a-kernel"><span class="mr-2">7.2.4 Learning a Kernel</span><a href="#724-learning-a-kernel" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>接下來我們要常識讓電腦自動學習kernel，為了簡單起見bias為0。(為什麼先sum再backward?)(https://www.youtube.com/watch?v=Q7KekwUricc)(https://dlvu.github.io/) 以下程式可以自動學習kernel</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre><td class="rouge-code"><pre><span class="c1"># Construct a two-dimensional convolutional layer with 1 output channel and a
# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">conv</span> <span class="kn">import</span> <span class="n">corr2d</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">3e-2</span> <span class="c1"># Learning rate
</span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LazyConv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">Y_hat</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_hat</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">conv2d</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">l</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update the kernel
</span>    <span class="n">conv2d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">conv2d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, loss </span><span class="si">{</span><span class="n">l</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv2d</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</pre></table></code></div></div><h3 id="pytorch-lazy-modules的用途"><span class="mr-2">PyTorch Lazy modules的用途:</span><a href="#pytorch-lazy-modules的用途" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>可以避免寫死in_features，對於動態的in_features來說比較方便 https://jarvislabs.ai/blogs/PyTorch-lazy-modules/</p><h3 id="725-cross-correlation-and-convolution"><span class="mr-2">7.2.5 Cross-Correlation and Convolution</span><a href="#725-cross-correlation-and-convolution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>再Deeplearning 使用的Convolution計算方式其實真正的名稱是Cross-Correlation，但是由於Cross-Correlation和Convolution的差別只在於kernerl上下和左右都互換，但對於traing的結果影響不大，所以依然稱為Convolution</p><h3 id="726-feature-map-and-receptive-field"><span class="mr-2">7.2.6 Feature Map and Receptive Field</span><a href="#726-feature-map-and-receptive-field" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>convolutional layer的輸出稱為Feature Map。<br /> receptive field則是影響輸出結果之前的所有元素。()</p><h2 id="73-padding-and-stride"><span class="mr-2">7.3 Padding and Stride</span><a href="#73-padding-and-stride" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>單純使用conv會使得後面layer的input快速變小。Padding可以解決這個狀況，相反的Stride則是用來快速讓輸入變小。</p><h3 id="731-padding"><span class="mr-2">7.3.1 Padding</span><a href="#731-padding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>padding和kernel關係的公式如下，假設row padding $p_h$, column padding $p_w$: \((n_h − k_h + p_h + 1 ) × (n_w − k_w + p_w + 1 )\) 我們可以藉由$p_h=k_h-1$和$p_w=k_w-1$讓輸入和輸出的長寬一樣。<br /> 如果$k$為奇數，則padding剛好可以填充到兩側，如果$k$為偶數則兩側的padding數量有一邊會多一個。</p><h3 id="732-stride"><span class="mr-2">7.3.2 Stride</span><a href="#732-stride" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>當高的stride為$s_h$寬的stride為$s_w$，則輸出為 \(\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor\)</p><h2 id="74-multiple-input-and-multiple-output-channels"><span class="mr-2">7.4 Multiple Input and Multiple Output Channels</span><a href="#74-multiple-input-and-multiple-output-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="741-multiple-input-channels"><span class="mr-2">7.4.1 Multiple Input Channels</span><a href="#741-multiple-input-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>當輸入的channel數量大於1的時候，每一個channel會需要至少一個kernel。假設現在輸入有3個channel，每一個channel分別對一個kernel之後相加結果就是輸出。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">corr2d_multi_in</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="c1"># Iterate through the 0th dimension (channel) of K first, then add them up
</span>    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">d2l</span><span class="p">.</span><span class="n">corr2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">]],</span>
<span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]])</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]]])</span>
<span class="n">corr2d_multi_in</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="c1"># tensor([[ 56., 72.],
# [104., 120.]])
</span></pre></table></code></div></div><h3 id="742-multiple-output-channels"><span class="mr-2">7.4.2 Multiple Output Channels</span><a href="#742-multiple-output-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>為了讓神經網路學到更多feature，我們嘗試讓同一層conv layer的kernel為三維而且第三個維度和輸入的channel數一樣$c_i\times k_h \times k_w$，例如輸入為3個channel，我們就設計kernel的第三個維度為3。由於每一組kernel最後的輸出都是一個channel。所以最後輸出的channel數就是kernel的組數。因此 convolution layer的維度就變成$c_o \times c_i \times k_h \times k_w$</p><ul><li><code class="language-plaintext highlighter-rouge">stack()</code>在這裡會把沿著指定的dimension把tensor堆疊<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">corr2d_multi_in_out</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
  <span class="c1"># Iterate through the 0th dimension of K, and each time, perform
</span>  <span class="c1"># cross-correlation operations with input X. All of the results are
</span>  <span class="c1"># stacked together
</span>  <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">corr2d_multi_in</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">K</span><span class="p">.</span><span class="n">shape</span>
<span class="c1">#torch.Size([3, 2, 2, 2])
</span><span class="n">corr2d_multi_in_out</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="c1"># tensor([[[ 56., 72.],
#         [104., 120.]],
#         [[ 76., 100.],
#         [148., 172.]],
#         [[ 96., 128.],
#         [192., 224.]]])
</span></pre></table></code></div></div></ul><h3 id="743-1--1-convolutional-layer"><span class="mr-2">7.4.3 1 × 1 Convolutional Layer</span><a href="#743-1--1-convolutional-layer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>1 × 1 Convolutional Layer唯一能夠影像的就是channel這個維度。 1 × 1 Convolutional Layer可以視為在每一個單獨的像素位置上的channel的fully conection layer。並且將channel數由$c_i$轉換成$c_o$。</p><h2 id="75-pooling"><span class="mr-2">7.5 Pooling</span><a href="#75-pooling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Pooling讓我們可以專注在影像比較大的區域而不會因為影像中細微的變化就影響輸出結果。</p><h3 id="751-maximum-pooling-and-average-pooling"><span class="mr-2">7.5.1 Maximum Pooling and Average Pooling</span><a href="#751-maximum-pooling-and-average-pooling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>池化層沒有任何參數，將window所有元素平均為Average Pooling，取出最大值則為Maximum Pooling。</p><h3 id="752-padding-and-stride"><span class="mr-2">7.5.2 Padding and Stride</span><a href="#752-padding-and-stride" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>跟conv layer一樣pooling也有padding和Stride，而由於pooling主要目的是從一個區域擷取資訊，所以深度學習框架預設將Stride的大小設定跟pooling window一樣大，不過我們還是可以自行修改大小。</p><h3 id="753-multiple-channels"><span class="mr-2">7.5.3 Multiple Channels</span><a href="#753-multiple-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>不同於conv layer，pooling對每一個channel是分開處理的，而不像conv layer會把其他channel的結果相加。因此pooling的輸入和輸出channel數目是一樣的。</p><h2 id="76-convolutional-neural-networks-lenet"><span class="mr-2">7.6 Convolutional Neural Networks (LeNet)</span><a href="#76-convolutional-neural-networks-lenet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>接下來介紹LeNet</p><h3 id="761-lenet"><span class="mr-2">7.6.1 LeNet</span><a href="#761-lenet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>下面是LeNet的Pytorch實作，這裡使用了<code class="language-plaintext highlighter-rouge">Xavier initialization</code></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">init_cnn</span><span class="p">(</span><span class="n">module</span><span class="p">):</span> <span class="c1">#@save
</span>    <span class="s">"""Initialize weights for CNNs."""</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">:</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LeNet</span><span class="p">(</span><span class="n">d2l</span><span class="p">.</span><span class="n">Classifier</span><span class="p">):</span> <span class="c1">#@save
</span>    <span class="s">"""The LeNet-5 model."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyConv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyConv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
</pre></table></code></div></div><h3 id="762-training"><span class="mr-2">7.6.2 Training</span><a href="#762-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>雖然CNN的參數比較少，但是計算量並不少，用GPU計算更適合。</p><h1 id="8-modern-convolutional-neural-networks">8 Modern Convolutional Neural Networks</h1><h2 id="81-deep-convolutional-neural-networks-alexnet"><span class="mr-2">8.1 Deep Convolutional Neural Networks (AlexNet)</span><a href="#81-deep-convolutional-neural-networks-alexnet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>硬體運算速度的提升增加了CNN可行性。</p><h3 id="811-representation-learning"><span class="mr-2">8.1.1 Representation Learning</span><a href="#811-representation-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>有一派的研究員相信圖片中的feature是可以讓電腦自己學習得到的。AlexNet在地層layer所學到的kernel跟傳統機器學習手工製作的feature很像。<br /> 推動CNN兩大因素別是”資料”和”電腦硬體”，過去的資料集都很小而且電腦運算速度很慢。</p><h3 id="812-alennet"><span class="mr-2">8.1.2 AlenNet</span><a href="#812-alennet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>AlexNet把sigmoid activation function改成ReLU activtion function。<br /> ReLU activation function，讓模型訓練變得更加容易。因為如果初始化的參數很接近0或1，sigmoid activation function的輸出值會很接近0，這使的反向傳播幾乎沒有作用。而ReLU activation function卻不會有這種情形。<br /> AlexNet在訓練的時候也大量的利用圖片擴增的技巧，這使得訓練結果不會overfitting。</p><h3 id="814-discussion"><span class="mr-2">8.1.4 Discussion</span><a href="#814-discussion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>AlexNet弱點是他最後的兩個layer佔用了非常大量的記憶體和運算，這對需要高速運算的場景非常不利。 另外一個可以注意的點是即使AlexNet的參數量遠大於資料及照片總數，AlexNet也幾乎沒有Overfitting。這是因為有用到現代化的正規化方法如Dropout。</p><h2 id="82-networks-using-blocks-vgg"><span class="mr-2">8.2 Networks Using Blocks (VGG)</span><a href="#82-networks-using-blocks-vgg" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>神經網絡架構的設計日益抽象化，研究人員從思考個別神經元逐漸轉向整個層面，然後到塊狀結構，即層的重複模式。十年後，這已經進一步發展到研究人員使用完整的訓練模型，將它們重新應用於不同但相關的任務。這種大型 pretrained models通常被稱為foundation models 。<br /> 這種Blocks的概念最早出現在VGG network，使用迴圈和子程序，可以在任何現代深度學習框架中輕鬆地在代碼中實現這些重複的結構。</p><h3 id="821-vgg-blocks"><span class="mr-2">8.2.1 VGG Blocks</span><a href="#821-vgg-blocks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>CNNs的基本構建塊是以下順序的序列：（i）帶填充的卷積層以保持解析度，（ii）如ReLU之類的非線性激活函數，（iii）如最大池化之類的池化層以減小解析度。這種方法的一個問題是空間解析度下降得相當迅速。特別是，這在所有維度（d）用完之前，對於網絡在卷積層上存在著 $log_2d$的硬限制。例如，在ImageNet的情況下，以這種方式不可能有超過8個卷積層。<br /> Simonyan和Zisserman（2014）的關鍵想法是使用連續的小卷層來取代一個大卷積層，例如兩個3x3的卷積層其實涵蓋的像素跟一個5x5一樣大，經過觀察利用連續小的卷積層的效果跟直接用一個大卷積層的效果相似。<br /> 堆疊3×3的卷積後來成為後來的深度網絡的黃金標準，直到最近由Liu等人（2022）重新審查了這個設計決策。</p><ul><li>VGG的構造:一連串的$3 \times 3$kernel且padding為1的conv layer最後接著一個$2 \times 2$ stride為2的max-pooling。下面程式實作一個VGG block，以$3 \times 3$kernel的數量num_convs和輸出channel數量out_channels作為參數。</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">vgg_block</span><span class="p">(</span><span class="n">num_convs</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_convs</span><span class="p">):</span>
        <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">LazyConv2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">())</span>
    <span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="822-vgg-network"><span class="mr-2">8.2.2 VGG Network</span><a href="#822-vgg-network" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>VGG Network可以拆成兩個部分，前半段是多個conv layer和pooling layer組成，後半段是fully connected layer所組成。</p><p>下面程式定義了VGG network，在這裡我們利用for迴圈將多個VGG block組合成VGG netowrk</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">d2l</span><span class="p">.</span><span class="n">Classifier</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="n">conv_blks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">num_convs</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span> <span class="ow">in</span> <span class="n">arch</span><span class="p">:</span>
            <span class="n">conv_blks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">vgg_block</span><span class="p">(</span><span class="n">num_convs</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="n">conv_blks</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">4096</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="mi">4096</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">net</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">d2l</span><span class="p">.</span><span class="n">init_cnn</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="83-network-in-network-nin"><span class="mr-2">8.3 Network in Network (NiN)</span><a href="#83-network-in-network-nin" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>VGG, LeNet, VGG都有一個共通的缺點。</p><ol><li>fully connected layer有非常大的參數量。<li>fully connected layer沒辦法放在模型最後面以外的地方。 Network in Network解決了上面兩個問題。他利用了像面兩個方式解決。<li>利用$1 \times 1$ conv layer來取代fully connected layer<li>在模型的最後面使用global average pooling NiN的好處是沒有fully connected layer因此參數量大大減少。<h3 id="831-nin-blocks"><span class="mr-2">8.3.1 NiN Blocks</span><a href="#831-nin-blocks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>由一個conv layer加上兩個$1 \times 1$ conv layer組成</p></ol><h2 id="84-multi-branch-networks-googlenet"><span class="mr-2">8.4 Multi-Branch Networks (GoogLeNet)</span><a href="#84-multi-branch-networks-googlenet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>GoogLeNet可以說是第一個將模型拆解成三個部位stem, body, head的模型。</p><h3 id="841-inception-blocks"><span class="mr-2">8.4.1 Inception Blocks</span><a href="#841-inception-blocks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Inception Blocks跟前面模型不一樣的是一個block有三個分支</p><h2 id="85-batch-normalization"><span class="mr-2">8.5 Batch Normalization</span><a href="#85-batch-normalization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Batch Normalization是一個能夠有效加速深度模型收斂的方法，與 residual blocks可以讓模型深度達到一百層。</p><h3 id="851-training-deep-networks"><span class="mr-2">8.5.1 Training Deep Networks</span><a href="#851-training-deep-networks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Batch normalization 可以被套用在單一個layer或是所有layer，如此一來對每一層的input都先做一次 normalization(也就是平均為0標準差為1)。 注意到如果我們套用batch normalization在minibatches為一的狀況下，我們不會學到任何東西，因為所有的 hidden unit都會變成0。<br /> 因此使用Batch normalization的時候batch size的大小變成十分重要。<br /> Batch normalization中scale parameter 和shift parameter是透過模型學習而得。</p><h3 id="852-batch-normalization-layers"><span class="mr-2">8.5.2 Batch Normalization Layers</span><a href="#852-batch-normalization-layers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h1 id="ch9-recurrent-neural-networkrnn">CH9 Recurrent Neural Network(RNN)</h1><p>在這之前我們的資料長度都是固定的，在這章將要學習如何讓模型學習長度不固定的序列資料。 RNN利用recurrent connection讓模型可以學習序列化的資料，可以把他想成一個迴授迴路</p><p>9.1 序列化資料 在此之前我們的特徵向量長度是固定的，而序列化資料的特徵向量是以時間排序而且長度不固定的資料。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/'>深度學習</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/book/" class="post-tag no-text-decoration" >book</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Dive+into+DeepLearning+-+Steven&url=https%3A%2F%2Fjenhaoyang.github.io%2F%2Fposts%2FDiveIntoDeepLearning%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Dive+into+DeepLearning+-+Steven&u=https%3A%2F%2Fjenhaoyang.github.io%2F%2Fposts%2FDiveIntoDeepLearning%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fjenhaoyang.github.io%2F%2Fposts%2FDiveIntoDeepLearning%2F&text=Dive+into+DeepLearning+-+Steven" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Windows-NTP%E6%A0%A1%E6%99%82/">Windows-NTP校時</a><li><a href="/posts/Gstreamer%E4%BD%BF%E7%94%A8%E7%AD%86%E8%A8%98/">Gstreamer使用筆記</a><li><a href="/posts/ubuntu%E8%A8%AD%E5%AE%9Ahome%E7%9B%AE%E9%8C%84%E5%88%B0%E5%AE%9A%E5%8F%A6%E4%B8%80%E9%A1%86%E7%A1%AC%E7%A2%9F/">Ubuntu設定home目錄到定另一顆硬碟</a><li><a href="/posts/Linux%E4%B8%8B%E7%9A%84Docker%E6%9B%B4%E6%94%B9image%E5%84%B2%E5%AD%98%E4%BD%8D%E7%BD%AE/">Linux下的Docker更改image儲存位置</a><li><a href="/posts/tensorrt%E7%B3%BB%E5%88%97-%E4%B8%80-%E7%92%B0%E5%A2%83%E8%A8%AD%E5%AE%9A/">TensorRT系列(一)環境設定</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/cmake/">cmake</a> <a class="post-tag" href="/tags/cpp/">cpp</a> <a class="post-tag" href="/tags/git/">git</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/deepstream/">deepstream</a> <a class="post-tag" href="/tags/peopleflow/">peopleflow</a> <a class="post-tag" href="/tags/pythoncpp/">pythoncpp</a> <a class="post-tag" href="/tags/ssh/">ssh</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%E5%AF%A6%E4%BD%9Cyolov1/"><div class="card-body"> <em class="small" data-ts="1667873880" data-df="ll" > Nov 8, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>實作yolov1</h3><div class="text-muted small"><p> 計算layer輸出 首先可以看到輸入的圖片是448 x 448，然後經過7 x 7 x 64 stripe 2 的conv layer，以及2 x 2 stripe 2 的Maxpool layer。 如果直接計算輸出的dimension，會發現計算有問題!!因為\({ 輸入寬度 - kernel寬度 \over stripe} \ne 224\)。查詢後發現如果去看yolov1.c...</p></div></div></a></div><div class="card"> <a href="/posts/%E5%AF%A6%E4%BD%9Cyolov2/"><div class="card-body"> <em class="small" data-ts="1667986260" data-df="ll" > Nov 9, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>實作yolov2(YOLO9000)</h3><div class="text-muted small"><p> yolov2 詳細說明 https://senliuy.gitbook.io/advanced-deep-learning/chapter1/yolo9000-better-faster-stronger Darknet-19與YOLOv2 目前Github上的YOLOv2已經被修改過，和論文上的描述並不一樣，必須參考最原始的cfg，而且必須要是voc的版本。 https://git...</p></div></div></a></div><div class="card"> <a href="/posts/Data-exploration-for-Object-Detection/"><div class="card-body"> <em class="small" data-ts="1676357100" data-df="ll" > Feb 14, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Data exploration for Object Detection</h3><div class="text-muted small"><p> 資料整體的品質 首先可以對整個資料集做一個初步的檢查，包含 瀏覽整份資料集 確認沒有嚴重有誤的照片(例如全黑的照片) 確認所有照片都能夠被電腦讀取(以免訓練到一半程式被中斷) 照片尺寸和深寬比 對於整份資料集，統計所有照片的深寬比和尺寸十分重要，這些將會影響anchor size 和 ratios。通常有三種情況 照片大小和深寬比都一樣: 只需要決定縮放比例 照...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/vscode-C%E9%96%8B%E7%99%BC%E7%92%B0%E5%A2%83/" class="btn btn-outline-primary" prompt="Older"><p>vscode-C開發環境</p></a> <a href="/posts/gdb-pretty-print/" class="btn btn-outline-primary" prompt="Newer"><p>gdb pretty print</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "jenhaoyang/jenhaoyang.github.io", "data-repo-id": "R_kgDOGONpkw", "data-category": "Announcements", "data-category-id": "DIC_kwDOGONpk84CPyux", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/jenhaoyang">Steven Yang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/cmake/">cmake</a> <a class="post-tag" href="/tags/cpp/">cpp</a> <a class="post-tag" href="/tags/git/">git</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/deepstream/">deepstream</a> <a class="post-tag" href="/tags/peopleflow/">peopleflow</a> <a class="post-tag" href="/tags/pythoncpp/">pythoncpp</a> <a class="post-tag" href="/tags/ssh/">ssh</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-YD5Z1Y7D6R"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-YD5Z1Y7D6R'); }); </script>
